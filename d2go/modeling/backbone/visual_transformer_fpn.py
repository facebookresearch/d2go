#!/usr/bin/env python3

import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

from detectron2.layers import ShapeSpec
from detectron2.modeling import BACKBONE_REGISTRY, Backbone, build_resnet_backbone
from mobile_cv.arch.layers import (
    NaiveSyncBatchNorm,
    NaiveSyncBatchNorm1d,
)

__all__ = ["build_resnet_vt_fpn_backbone", "VT_FPN"]


def conv3x3(inplanes, out_planes, stride=1, groups=1, dilation=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(inplanes, out_planes, kernel_size=3, stride=stride,
                     padding=dilation, groups=groups, bias=False,
                     dilation=dilation)


def conv1x1(inplanes, out_planes, stride=1, groups=1):
    """1x1 convolution"""
    return nn.Conv2d(inplanes, out_planes, kernel_size=1, stride=stride,
                     groups=groups, bias=False)


def conv1x1_1d(inplanes, out_planes, stride=1, groups=1):
    """1x1 convolution"""
    return nn.Conv1d(inplanes, out_planes, kernel_size=1, stride=stride,
                     groups=groups, bias=False)


def get_norm(norm, dim):
    assert dim in [1, 2]
    assert norm in ['', 'BN', 'SyncBN']
    if norm == 'BN':
        return nn.BatchNorm1d if dim == 1 else nn.BatchNorm2d
    elif norm == 'SyncBN':
        return NaiveSyncBatchNorm1d if dim == 1 else NaiveSyncBatchNorm
    elif norm == '':
        return nn.Identity


class MatMul(nn.Module):
    """A wrapper class such that we can count the FLOPs of matmul
    """
    def __init__(self):
        super(MatMul, self).__init__()

    def forward(self, A, B):
        return torch.matmul(A, B)


class Transformer(nn.Module):
    def __init__(self, token_c, t_layer=1, head=16, kqv_groups=1,
                 norm_layer_1d=nn.Identity, norm_layer_2d=nn.Identity):
        super(Transformer, self).__init__()

        self.k_conv = nn.ModuleList()
        self.q_conv = nn.ModuleList()
        self.v_conv = nn.ModuleList()
        self.kqv_bn = nn.ModuleList()
        self.kq_matmul = nn.ModuleList()
        self.kqv_matmul = nn.ModuleList()
        self.ff_conv = nn.ModuleList()
        for _ in range(t_layer):
            self.k_conv.append(nn.Sequential(
                conv1x1_1d(token_c, token_c // 2, groups=kqv_groups),
                norm_layer_1d(token_c // 2)
            ))
            self.q_conv.append(nn.Sequential(
                conv1x1_1d(token_c, token_c // 2, groups=kqv_groups),
                norm_layer_1d(token_c // 2)
            ))
            self.v_conv.append(nn.Sequential(
                conv1x1_1d(token_c, token_c, groups=kqv_groups),
                norm_layer_1d(token_c)
            ))
            self.kq_matmul.append(MatMul())
            self.kqv_matmul.append(MatMul())
            self.kqv_bn.append(norm_layer_1d(token_c))
            # zero-init
            nn.init.constant_(self.kqv_bn[-1].weight, 0)
            self.ff_conv.append(nn.Sequential(
                conv1x1_1d(token_c, token_c),
                norm_layer_1d(token_c)
            ))
            # initialize the bn weight to zero to improves the training
            # stability.
            nn.init.constant_(self.ff_conv[-1][1].weight, 0)

        self.token_c = token_c
        self.t_layer = t_layer
        self.head = head

    def forward(self, x):
        N = x.shape[0]
        for _idx in range(self.t_layer):
            k = self.k_conv[_idx](x).view(
                N, self.head, self.token_c // 2 // self.head, -1)
            q = self.q_conv[_idx](x).view(
                N, self.head, self.token_c // 2 // self.head, -1)
            v = self.v_conv[_idx](x).view(
                N, self.head, self.token_c // self.head, -1)
            # N, h, L, C/h * N, h, C/h, L -> N, h, L, L
            kq = self.kq_matmul[_idx](k.permute(0, 1, 3, 2), q)
            # N, h, L, L
            kq = F.softmax(kq / np.sqrt(self.token_c / 2 / self.head), dim=2)
            # N, h, C/h, L * N, h, L, L -> N, h, C/h, L
            kqv = self.kqv_matmul[_idx](v, kq).view(N, self.token_c, -1)
            kqv = self.kqv_bn[_idx](kqv)
            x = x + kqv
            x = x + self.ff_conv[_idx](x)

        return x


class VT_FPN(Backbone):

    def __init__(
        self, cfg, bottom_up, in_features, out_channels, top_block=None,
    ):
        """
        Args:
            cfg: cfgNode that defines the behavior of visual-transformer-based
                FPN.
            bottom_up (Backbone): module representing the bottom up subnetwork.
                Must be a subclass of :class:`Backbone`. The multi-scale feature
                maps generated by the bottom up network, and listed in
                `in_features`, are used to generate FPN levels.
            in_features (list[str]): names of the input feature maps coming
                from the backbone to which FPN is attached. For example, if the
                backbone produces ["res2", "res3", "res4"], order must be from
                high to low resolution.
            out_channels (int): number of channels in the output feature maps.
            top_block (nn.Module or None): if provided, an extra operation will
                be performed on the output of the last (smallest resolution)
                FPN output, and the result will extend the result list. The top_block
                further downsamples the feature map. It must have an attribute
                "num_levels", meaning the number of extra FPN levels added by
                this block, and "in_feature", which is a string representing
                its input feature (e.g., p5).
        """
        super(VT_FPN, self).__init__()
        assert isinstance(bottom_up, Backbone)

        # Feature map strides and channels from the bottom up network (e.g. ResNet)
        input_shapes = bottom_up.output_shape()
        in_strides = [input_shapes[f].stride for f in in_features]
        in_dims = [input_shapes[f].channels for f in in_features]
        out_dims = [out_channels for _ in range(len(in_dims))]

        _assert_strides_are_log2_contiguous(in_strides)
        # === Visual-transformer blocks ===
        token_ls = cfg.MODEL.VT_FPN.TOKEN_LS
        self.visual_transformers = nn.ModuleList()
        for _idx in range(cfg.MODEL.VT_FPN.LAYERS):
            self.visual_transformers.append(
                VisualTransformer(
                    token_ls, cfg.MODEL.VT_FPN.TOKEN_C,
                    in_dims if (_idx == 0) else out_dims, out_dims,
                    pos_hws=cfg.MODEL.VT_FPN.POS_HWS,
                    pos_n_downsample=cfg.MODEL.VT_FPN.POS_N_DOWNSAMPLE,
                    init_x_t=(_idx == 0), head=cfg.MODEL.VT_FPN.HEADS,
                    min_group_planes=cfg.MODEL.VT_FPN.MIN_GROUP_PLANES,
                    norm_layer_1d=get_norm(cfg.MODEL.VT_FPN.NORM, dim=1),
                    norm_layer_2d=get_norm(cfg.MODEL.VT_FPN.NORM, dim=2)
                )
            )

        self.in_features = in_features
        self.bottom_up = bottom_up
        # Return feature names are "p<stage>", like ["p2", "p3", ..., "p6"]
        self._out_feature_strides = {
            "p{}".format(int(math.log2(s))): s for s in in_strides}
        stage = int(math.log2(in_strides[-1]))
        self.top_block = top_block
        # top block output feature maps.
        if self.top_block is not None:
            for s in range(stage, stage + self.top_block.num_levels):
                self._out_feature_strides["p{}".format(s + 1)] = 2 ** (s + 1)

        self._out_features = list(self._out_feature_strides.keys())
        self._out_feature_channels = {k: out_channels for k in self._out_features}
        self._size_divisibility = in_strides[-1]

    @property
    def size_divisibility(self):
        return self._size_divisibility

    def forward(self, x):
        """
        Args:
            input (dict[str->Tensor]): mapping feature map name (e.g., "res5") to
                feature map tensor for each feature level in high to low
                resolution order.

        Returns:
            dict[str->Tensor]:
                mapping from feature map name to FPN feature map tensor in high
                to low resolution order. Returned feature names follow the FPN
                paper convention: "p<stage>", where stage has stride = 2 **
                stage e.g., ["p2", "p3", ..., "p6"].
        """
        # Reverse feature maps into top-down order (from low to high resolution)
        bottom_up_features = self.bottom_up(x)
        x = [bottom_up_features[f] for f in self.in_features]

        x_t = [None for _ in range(len(x))]
        for vt in self.visual_transformers:
            x, x_t = vt(x, x_t)

        if self.top_block is not None:
            top_block_in_feature = bottom_up_features.get(
                self.top_block.in_feature, None)
            if top_block_in_feature is None:
                top_block_in_feature = x[
                    self._out_features.index(self.top_block.in_feature)]
            x.extend(self.top_block(top_block_in_feature))

        assert len(self._out_features) == len(x)
        return dict(zip(self._out_features, x))

    def output_shape(self):
        return {
            name: ShapeSpec(
                channels=self._out_feature_channels[name],
                stride=self._out_feature_strides[name]
            )
            for name in self._out_features
        }


class VisualTransformer(nn.Module):
    def __init__(self, token_ls, token_c, input_dims, output_dims,
                 pos_hws='', pos_n_downsample='', init_x_t=False, head=16,
                 min_group_planes=64, norm_layer_1d=nn.Identity,
                 norm_layer_2d=nn.Identity, **kwargs):
        super(VisualTransformer, self).__init__()

        assert len(token_ls) == len(input_dims), '{} vs {}'.format(
            len(token_ls), len(input_dims))
        assert len(token_ls) == len(output_dims), '{} vs {}'.format(
            len(token_ls), len(output_dims))

        self.feature_blocks = nn.ModuleList()
        self.tokenizers = nn.ModuleList()
        self.transformer = Transformer(
            token_c, norm_layer_1d=norm_layer_1d, norm_layer_2d=norm_layer_2d,
            **kwargs)
        self.projectors = nn.ModuleList()

        if len(pos_hws) == 0:
            pos_hws = [None for _ in range(len(input_dims))]
            pos_n_downsample = [0 for _ in range(len(input_dims))]
        for _idx in range(len(input_dims)):
            if input_dims[_idx] == output_dims[_idx]:
                feature_block = nn.Identity()
            else:
                feature_block = nn.Sequential(
                    conv1x1(input_dims[_idx], output_dims[_idx]),
                    norm_layer_2d(output_dims[_idx]),
                )
            self.feature_blocks.append(feature_block)

            self.tokenizers.append(
                Tokenizer(
                    token_ls[_idx],
                    token_c,
                    input_dims[_idx],
                    init_x_t,
                    pos_hw=pos_hws[_idx],
                    pos_n_downsample=pos_n_downsample[_idx],
                    head=head,
                    min_group_planes=min_group_planes,
                    norm_layer_1d=norm_layer_1d,
                    norm_layer_2d=norm_layer_2d,
                )
            )

            self.projectors.append(
                Projector(
                    token_c, output_dims[_idx], head=head,
                    min_group_planes=min_group_planes,
                    norm_layer_1d=norm_layer_1d, norm_layer_2d=norm_layer_2d)
            )

        self.token_ls = token_ls

    def forward(self, in_feature_list, in_token_list):
        tokens = []
        for _idx in range(len(in_feature_list)):
            tokens.append(
                self.tokenizers[_idx](
                    in_feature_list[_idx],
                    in_token_list[_idx]
                )
            )

        tokens = self.transformer(torch.cat(tokens, dim=2))

        out_token_list = []
        tidx = 0
        for _idx in range(len(self.token_ls)):
            out_token_list.append(
                tokens[:, :, tidx:tidx + self.token_ls[_idx]])
            tidx += self.token_ls[_idx]
        assert tidx == tokens.shape[2]

        out_feature_list = []
        for _idx in range(len(in_feature_list)):
            out_feature_list.append(
                self.projectors[_idx](
                    self.feature_blocks[_idx](in_feature_list[_idx]),
                    out_token_list[_idx]
                )
            )

        return out_feature_list, out_token_list


class Projector(nn.Module):
    def __init__(self, token_c, planes, head=16, min_group_planes=64,
                 norm_layer_1d=nn.Identity, norm_layer_2d=nn.Identity):
        super(Projector, self).__init__()

        if token_c != planes:
            self.proj_value_conv = conv1x1_1d(token_c, planes)
        else:
            self.proj_value_conv = nn.Identity()
        self.proj_key_conv = nn.Sequential(
            conv1x1_1d(token_c, planes),
            norm_layer_1d(planes)
        )
        self.proj_query_conv = nn.Sequential(
            conv1x1(planes, planes,
                    groups=min(head, planes // min_group_planes)),
            norm_layer_2d(planes)
        )
        self.proj_kq_matmul = MatMul()
        self.proj_matmul = MatMul()
        self.proj_bn = norm_layer_2d(planes)
        # zero-init
        nn.init.constant_(self.proj_bn.weight, 0)

        self.head = head

    def forward(self, x, x_t):
        N, _, L = x_t.shape
        h = self.head
        # -> N, h, C/h, L
        proj_v = self.proj_value_conv(x_t).view(N, h, -1, L)
        # -> N, h, C/h, L
        proj_k = self.proj_key_conv(x_t).view(N, h, -1, L)
        proj_q = self.proj_query_conv(x)
        N, C, H, W = proj_q.shape
        # -> N, h, HW, c/H
        proj_q = proj_q.view(N, h, C // h, H * W).permute(0, 1, 3, 2)
        # N, h, HW, C/h * N, h, C/h, L -> N, h, HW, L
        proj_coef = F.softmax(
            self.proj_kq_matmul(proj_q, proj_k) / np.sqrt(C / h), dim=3)

        # N, h, C/h, L * N, h, L, HW -> N, h, C/h, HW
        x_p = self.proj_matmul(proj_v, proj_coef.permute(0, 1, 3, 2))
        # -> N, C, H, W
        _, _, H, W = x.shape
        x_p = self.proj_bn(x_p.view(N, -1, H, W))

        x = x + x_p

        return x


class PosEncoder(nn.Module):
    def __init__(self, dim, pos_hw, num_downsample,
                 norm_layer_1d=nn.Identity, norm_layer_2d=nn.Identity):
        super(PosEncoder, self).__init__()
        self.pos_hw = pos_hw

        ds_conv = []
        for _idx in range(num_downsample):
            ds_conv.append(
                nn.Sequential(
                    conv3x3(dim, dim, stride=2),
                    norm_layer_2d(dim)
                )
            )
        ds_conv.append(nn.Sequential(conv1x1(dim, 1), norm_layer_2d(1)))
        self.ds_conv = nn.Sequential(*ds_conv)

        pos_dim = pos_hw * pos_hw // (4**num_downsample)
        self.pos_conv = nn.Sequential(
            conv1x1_1d(pos_dim, pos_dim),
            norm_layer_1d(pos_dim)
        )
        self.pos_dim = pos_dim

    def forward(self, token_coef, original_hw):
        H, W = original_hw
        N, h, HW, L = token_coef.shape
        # Resize token_coef using interpolation to the target size
        token_coef = token_coef.permute(0, 3, 1, 2).contiguous().view(
            N * L, h, H, W)
        token_coef = F.interpolate(
            token_coef, size=(self.pos_hw, self.pos_hw), mode='bilinear')
        # Downsample the token_coef using convolutions
        token_coef = self.ds_conv(token_coef)
        # Convert to positional encoding
        token_coef = token_coef.view(N, L, -1).permute(0, 2, 1)
        return self.pos_conv(token_coef)


class Tokenizer(nn.Module):
    def __init__(self, token_l, token_c, planes, init_x_t, pos_hw=None,
                 pos_n_downsample=0, head=16, min_group_planes=64,
                 norm_layer_1d=nn.Identity, norm_layer_2d=nn.Identity):
        super(Tokenizer, self).__init__()

        if init_x_t:
            self.conv_gather = nn.Sequential(
                conv1x1(planes, token_l),
                norm_layer_2d(token_l)
            )
        else:
            self.conv_query = nn.Sequential(
                conv1x1_1d(token_c, planes),
                norm_layer_1d(planes)
            )
            self.query_matmul = MatMul()
            self.conv_key = nn.Sequential(
                conv1x1(planes, planes,
                        groups=min(head, planes // min_group_planes)),
                norm_layer_2d(planes)
            )

        self.conv_value = nn.Sequential(
            conv1x1(planes, planes,
                    groups=min(head, planes // min_group_planes)),
            norm_layer_2d(planes)
        )

        self.gather_matmul = MatMul()
        self.gather_bn = norm_layer_1d(planes)

        token_in_planes = planes
        if pos_hw is not None:
            self.pos_encoding = PosEncoder(
                1 if init_x_t else head, pos_hw, pos_n_downsample,
                norm_layer_1d, norm_layer_2d)
            token_in_planes += self.pos_encoding.pos_dim

        self.conv_token = nn.Sequential(
            conv1x1_1d(token_in_planes, token_c),
            norm_layer_1d(token_c)
        )

        self.init_x_t = init_x_t
        self.head = head

    def forward(self, x, x_t):

        if self.init_x_t:
            # If x_t has not been created before, use a convolution with static
            # weights to extract it.
            # N, L, H, W
            token_coef = self.conv_gather(x)
            N, L, H, W = token_coef.shape
            # -> N, 1, L, HW
            token_coef = token_coef.view(N, 1, L, H * W)
            # -> N, 1, HW, L
            token_coef = token_coef.permute(0, 1, 3, 2).contiguous()
            token_coef = token_coef / np.sqrt(x.shape[1])
        else:
            L = x_t.shape[2]
            x_t, x_t_res = x_t[:, :, :L // 2], x_t[:, :, L // 2:]
            # If x_t is computed from the previous layer, generate the query
            # from the previous x_t to extract new tokens from the current input.
            query = self.conv_query(x_t)
            N, C, L = query.shape
            # N, h, C/h, L
            query = query.view(N, self.head, C // self.head, L)
            N, C, H, W = x.shape
            # N, h, C/h, HW
            key = self.conv_key(x).view(
                N, self.head, C // self.head, H * W)
            # N, h, HW, C/h * N, h, C/h, L -> N, h, HW, L
            token_coef = self.query_matmul(key.permute(0, 1, 3, 2), query)
            token_coef = token_coef / np.sqrt(C / self.head)

        N, C, H, W = x.shape
        token_coef = F.softmax(token_coef, dim=2)
        value = self.conv_value(x).view(N, self.head, C // self.head, H * W)
        # N, h, C/h, HW * N, h, HW, L -> N, h, C/h, L -> N, C, L
        tokens = self.gather_matmul(value, token_coef).view(N, C, -1)
        tokens = self.gather_bn(tokens)

        # == add position encoding to the visual tokens ==
        if hasattr(self, 'pos_encoding'):
            _, _, H, W = x.shape
            pos_encoding = self.pos_encoding(token_coef, (H, W))
            tokens = torch.cat((tokens, pos_encoding), dim=1)

        if self.init_x_t:
            x_t = self.conv_token(tokens)
        else:
            x_t = torch.cat((x_t_res, self.conv_token(tokens)), dim=2)

        return x_t


def _assert_strides_are_log2_contiguous(strides):
    """
    Assert that each stride is 2x times its preceding stride, i.e. "contiguous
    in log2".
    """
    for i, stride in enumerate(strides[1:], 1):
        assert stride == 2 * strides[i - 1], \
            "Strides {} {} are not log2 contiguous".format(
                stride, strides[i - 1])


class LastLevelMaxPool(nn.Module):
    """
    This module is used in the original FPN to generate a downsampled
    P6 feature from P5.
    """

    def __init__(self):
        super().__init__()
        self.num_levels = 1
        self.in_feature = "p5"

    def forward(self, x):
        return [F.max_pool2d(x, kernel_size=1, stride=2, padding=0)]


# pyre-fixme[56]: Decorator `detectron2.modeling.BACKBONE_REGISTRY.register(...)`
#  could not be called, because its type `Optional[object]` is not callable.
@BACKBONE_REGISTRY.register()
def build_resnet_vt_fpn_backbone(cfg, input_shape: ShapeSpec):
    """
    Args:
        cfg: a detectron2 CfgNode

    Returns:
        backbone (Backbone): backbone module, must be a subclass of :class:`Backbone`.
    """
    bottom_up = build_resnet_backbone(cfg, input_shape)
    in_features = cfg.MODEL.VT_FPN.IN_FEATURES
    out_channels = cfg.MODEL.VT_FPN.OUT_CHANNELS
    backbone = VT_FPN(
        cfg,
        bottom_up=bottom_up,
        in_features=in_features,
        out_channels=out_channels,
    )
    return backbone


# pyre-fixme[56]: Decorator `detectron2.modeling.BACKBONE_REGISTRY.register(...)`
#  could not be called, because its type `Optional[object]` is not callable.
@BACKBONE_REGISTRY.register()
def build_resnet_vt_fpn_det_backbone(cfg, input_shape: ShapeSpec):
    """
    Args:
        cfg: a detectron2 CfgNode

    Returns:
        backbone (Backbone): backbone module, must be a subclass of :class:`Backbone`.
    """
    bottom_up = build_resnet_backbone(cfg, input_shape)
    in_features = cfg.MODEL.VT_FPN.IN_FEATURES
    out_channels = cfg.MODEL.VT_FPN.OUT_CHANNELS
    backbone = VT_FPN(
        cfg,
        bottom_up=bottom_up,
        in_features=in_features,
        out_channels=out_channels,
        top_block=LastLevelMaxPool(),
    )
    return backbone
