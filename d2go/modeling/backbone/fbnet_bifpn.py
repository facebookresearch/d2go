#!/usr/bin/env python3

import logging
import math

import torch.nn as nn
from detectron2.layers import ShapeSpec
from detectron2.modeling import BACKBONE_REGISTRY, Backbone
from detectron2.modeling.backbone.fpn import LastLevelMaxPool, LastLevelP6P7
from mobile_cv.arch.fbnet_v2 import fbnet_builder as mbuilder
from mobile_cv.arch.fbnet_v2.fbnet_fpn import FBNetFPNBuilder
from mobile_cv.arch.fbnet_v2.modeldef_registry import FBNetV2ModelArch

from .fbnet_v2 import build_fbnet_backbone, _merge_fbnetv2_arch_def


logger = logging.getLogger(__name__)


class FBNetBiFPN(Backbone):
    """
    This module implements a flexible Feature Pyramid Network.
    It creates pyramid features built on top of some input feature maps.
    """

    def __init__(
        self,
        bottom_up,
        in_features,
        builder,
        arch_def,
        top_block=None,
        top_block_before_fpn=False,
        top_block_out_channels=None,
    ):
        """
        Args:
            bottom_up (Backbone): module representing the bottom up subnetwork.
                Must be a subclass of :class:`Backbone`. The multi-scale feature
                maps generated by the bottom up network, and listed in `in_features`,
                are used to generate FPN levels.
            in_features (list[str]): names of the input feature maps coming
                from the backbone to which FPN is attached. For example, if the
                backbone produces ["res2", "res3", "res4"], any *contiguous* sublist
                of these may be used; order must be from high to low resolution.
            builder (FBNetFPNBuilder): model builder for FPN
            arch_def (list[dict]):
        """
        super(FBNetBiFPN, self).__init__()
        assert isinstance(bottom_up, Backbone)

        # Feature map strides and channels from the bottom up network:
        # high res to low res
        input_shapes = bottom_up.output_shape()
        in_strides = [input_shapes[f].stride for f in in_features]
        in_channels = [input_shapes[f].channels for f in in_features]

        self.top_block = top_block
        self.in_features = in_features
        self.bottom_up = bottom_up

        self.top_block_before_fpn = top_block_before_fpn
        if (self.top_block is not None) and self.top_block_before_fpn:
            assert top_block_out_channels is not None
            stage = int(math.log2(in_strides[-1]))
            for s in range(stage, stage + self.top_block.num_levels):
                in_strides.append(2 ** (s + 1))
                in_channels.append(top_block_out_channels)

        _assert_strides_are_log2_contiguous(in_strides)

        self._build_fpn(in_channels, builder, arch_def)

        # Return feature names are "p<stage>", like ["p2", "p3", ..., "p6"]
        self._out_feature_strides = {
            "p{}".format(int(math.log2(s))): s for s in in_strides
        }
        # top block output feature maps.
        out_channels = self.fpn[-1].output_channels
        if self.top_block is not None and not self.top_block_before_fpn:
            stage = int(math.log2(in_strides[-1]))
            for s in range(stage, stage + self.top_block.num_levels):
                self._out_feature_strides["p{}".format(s + 1)] = 2 ** (s + 1)
                out_channels.append(out_channels[-1])

        self._out_features = list(self._out_feature_strides.keys())
        # self._out_feature_channels = {k: out_channels for k in self._out_features}
        self._out_feature_channels = {
            k: chl for k, chl in zip(self._out_features, out_channels)
        }
        self._size_divisibility = in_strides[-1]

    def _build_fpn(self, in_channels, builder, arch_def):
        """
        Args:
            in_channels (list[int]): list of input channels to FPN, high res to low res
            arch_def (list[dict]): list of dicts with each dict corresponding to one
                pass (low -> high or high -> low) of FPN
        """
        self.fpn = nn.ModuleList()
        prev_channels, skip_channels = in_channels, [None for _ in in_channels]
        for fpn_def in arch_def:
            combiner_path = fpn_def.get("combiner_path", "high_res")
            if combiner_path == "low_res":
                prev_channels = prev_channels[::-1]
                skip_channels = skip_channels[::-1]
            fpn_in_channels = []
            for prev_channel, skip_channel in zip(prev_channels, skip_channels):
                fpn_in_channels.extend([prev_channel, skip_channel])
            fpn_module = builder.build_model(fpn_in_channels, fpn_def)
            self.fpn.append(fpn_module)
            prev_channels, skip_channels = fpn_module.output_channels, prev_channels
            if combiner_path == "low_res":
                skip_channels = skip_channels[::-1]
        return

    @property
    def size_divisibility(self):
        return self._size_divisibility

    def output_shape(self):
        return {
            name: ShapeSpec(
                channels=self._out_feature_channels[name],
                stride=self._out_feature_strides[name],
            )
            for name in self._out_features
        }

    def forward(self, x):
        """
        Args:
            input (dict[str->Tensor]): mapping feature map name (e.g., "res5")
                to feature map tensor for each feature level in high to low
                resolution order.

        Returns:
            dict[str->Tensor]:
                mapping from feature map name to FPN feature map tensor
                in high to low resolution order. Returned feature names follow the FPN
                paper convention: "p<stage>", where stage has stride = 2 ** stage e.g.,
                ["p2", "p3", ..., "p6"].
        """
        # Reverse feature maps into top-down order (from low to high resolution)
        bottom_up_features = self.bottom_up(x)
        x = [bottom_up_features[f] for f in self.in_features]

        if self.top_block is not None and self.top_block_before_fpn:
            top_block_in_feature = bottom_up_features.get(self.top_block.in_feature, None)
            if top_block_in_feature is None:
                top_block_in_feature = x[self._out_features.index(self.top_block.in_feature)]
            x.extend(self.top_block(top_block_in_feature))

        prev_inputs, skip_inputs = x, [None for _ in range(len(x))]
        for fpn_module in self.fpn:
            if fpn_module.combiner_path == "low_res":
                prev_inputs = prev_inputs[::-1]
                skip_inputs = skip_inputs[::-1]

            fpn_inputs = []
            for prev_input, skip_input in zip(prev_inputs, skip_inputs):
                fpn_inputs.extend([prev_input, skip_input])

            fpn_outputs = fpn_module(fpn_inputs)
            skip_inputs = prev_inputs
            prev_inputs = fpn_outputs
            if fpn_module.combiner_path == "low_res":
                skip_inputs = skip_inputs[::-1]

        results = fpn_outputs
        if self.top_block is not None and not self.top_block_before_fpn:
            top_block_in_feature = bottom_up_features.get(
                self.top_block.in_feature, None
            )
            if top_block_in_feature is None:
                top_block_in_feature = results[
                    self._out_features.index(self.top_block.in_feature)
                ]
            results.extend(self.top_block(top_block_in_feature))
        assert len(self._out_features) == len(results)
        return dict(zip(self._out_features, results))


def _assert_strides_are_log2_contiguous(strides):
    """
    Assert that each stride is 2x times its preceding stride, i.e. "contiguous in log2".
    """
    for i, stride in enumerate(strides[1:], 1):
        assert (
            stride == 2 * strides[i - 1]
        ), "Strides {} {} are not log2 contiguous".format(stride, strides[i - 1])


def _get_builder_norm_args(cfg):
    norm_name = cfg.MODEL.BIFPN.NORM
    norm_args = {"name": norm_name}
    assert all(isinstance(x, dict) for x in cfg.MODEL.BIFPN.NORM_ARGS)
    for dic in cfg.MODEL.BIFPN.NORM_ARGS:
        norm_args.update(dic)
    return norm_args


def _get_fpn_builder_and_arch_def(cfg):
    arch = cfg.MODEL.FBNET_V2.ARCH
    arch_def = cfg.MODEL.FBNET_V2.ARCH_DEF
    assert (arch and not arch_def) ^ (not arch and arch_def), (
        "Only allow one unset node between MODEL.FBNET_V2.ARCH ({}) and MODEL.FBNET_V2.ARCH_DEF ({})"
        .format(arch, arch_def)
    )
    raw_arch_def = FBNetV2ModelArch.get(arch) if arch else _merge_fbnetv2_arch_def(cfg)

    arch_def = raw_arch_def["bifpn"]

    # NOTE: one can store extra information in arch_def to configurate FBNetBuilder,
    # after this point, builder and arch_def will become independent.
    basic_args = raw_arch_def.pop("basic_args", {})

    builder = mbuilder.FBNetBuilder(
        width_ratio=cfg.MODEL.BIFPN.SCALE_FACTOR,
        width_divisor=cfg.MODEL.BIFPN.WIDTH_DIVISOR,
        bn_args=_get_builder_norm_args(cfg),
    )
    builder.add_basic_args(**basic_args)
    builder_fpn = FBNetFPNBuilder(builder)
    return builder_fpn, arch_def


@BACKBONE_REGISTRY.register()
def FBNetV2BiFpnBackbone(cfg, _):
    builder, arch_def = _get_fpn_builder_and_arch_def(cfg)
    bottom_up = build_fbnet_backbone(cfg)
    top_block = LastLevelMaxPool()
    backbone = FBNetBiFPN(
        bottom_up=bottom_up,
        in_features=cfg.MODEL.FPN.IN_FEATURES,
        builder=builder,
        arch_def=arch_def,
        top_block=top_block,
        top_block_before_fpn=cfg.MODEL.BIFPN.TOP_BLOCK_BEFORE_FPN,
        top_block_out_channels=cfg.MODEL.FPN.OUT_CHANNELS
    )

    return backbone


@BACKBONE_REGISTRY.register()
def FBNetV2RetinaNetBiFpnBackbone(cfg, _):
    builder, arch_def = _get_fpn_builder_and_arch_def(cfg)
    bottom_up = build_fbnet_backbone(cfg)
    in_channels_p6p7 = bottom_up.output_shape()[cfg.MODEL.FPN.IN_FEATURES[-1]].channels
    top_block = LastLevelP6P7(in_channels_p6p7, cfg.MODEL.FPN.OUT_CHANNELS)
    top_block.in_feature = cfg.MODEL.FPN.IN_FEATURES[-1]
    backbone = FBNetBiFPN(
        bottom_up=bottom_up,
        in_features=cfg.MODEL.FPN.IN_FEATURES,
        builder=builder,
        arch_def=arch_def,
        top_block=top_block,
        top_block_before_fpn=cfg.MODEL.BIFPN.TOP_BLOCK_BEFORE_FPN,
        top_block_out_channels=cfg.MODEL.FPN.OUT_CHANNELS
    )

    return backbone
